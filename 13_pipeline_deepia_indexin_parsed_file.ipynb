{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "dy3zqm_ra_FA",
    "outputId": "1854048e-503f-4dd4-e8c7-f10ced119ebe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGts0QBdbdwJ"
   },
   "source": [
    "###para la inferencia podemos o espera un intervalod e tiempo para crear un eventcount_vector o sera que creamos una pequena memoria con cierta cantidad de lieas de logs hacia atras y vamos creando vectores y probando.\n",
    "\n",
    "Creo que no tiene sentido hacer inferencia de linea en linea\n",
    " \n",
    "- TODO: pensar y validar en codigo como hacer la inferencia de vainas nuevas\n",
    " \n",
    "\n",
    "- definir cuales son los indicadores que se mostraran a dayco, tiempo de procesamiento, memoria, eficiencia delproceso\n",
    "\n",
    "- justificar con argumentos porque el proceso funcionara con la data dayco igual que con la data de prueba\n",
    "\n",
    "- como hacer en tiempo de inferencia, podemos simular que un archivo se va llenando de logs???revisar como cambia un archivo de logs en el logparser???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8854
    },
    "colab_type": "code",
    "id": "-NvOzNVoj69w",
    "outputId": "9beff484-bf9b-49c5-c160-990fbb7485ee"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!apt-get update\n",
    "!apt-get install -y wget bzip2\n",
    "!apt-get install -y gcc perl git\n",
    "!rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "!cd /\n",
    "!mkdir anaconda\n",
    "!cd anaconda\n",
    "!wget https://repo.anaconda.com/archive/Anaconda2-5.2.0-Linux-x86_64.sh\n",
    "!bash Anaconda2-5.2.0-Linux-x86_64.sh \\n\\n\\n\\n\\n\\n\\n\\n\\n\n",
    "!source ~/.bashrc\n",
    "!cd ..\n",
    "!rm -r anaconda \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaO9yI6qXnm-"
   },
   "source": [
    "## clonamos loglizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "GdTV71kLbbDn",
    "outputId": "7b124f04-4bf8-4698-ec07-fb4bc4cf2e30"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/elieser1101/loglizer.git /loglizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "_fNd9TcyR_4v",
    "outputId": "fe0ffadb-0c97-451d-a273-c07a44871df2"
   },
   "outputs": [],
   "source": [
    "#os.chdir('/loglizer')\n",
    "#!git pull origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuAkicr6XvMR"
   },
   "source": [
    "## iniciamos submodulo de git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "2_1OwvMocJg2",
    "outputId": "782244a4-ce6b-4bc2-f628-4d86161b79b2"
   },
   "outputs": [],
   "source": [
    "#os.chdir('/loglizer')\n",
    "#!git submodule update --init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "fm5BiUHkiJ_Z",
    "outputId": "3e52ac81-f3b0-4290-e932-f793122f8bf2"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbViK16YX0sM"
   },
   "source": [
    "## importamos sys y agregamos algunas rutas necesarias para el logparser al path de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3ls7oU5oReM"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#TODO: esto se debe incluir en repo, ver si hay una manera correcta o si es haciendo append del path!\n",
    "#os.chdir('/loglizer')\n",
    "repo_path = '/home/us1/git/loglizer'\n",
    "sys.path.append(repo_path)#logsig __init__.py\n",
    "sys.path.append(repo_path + '/logparser/logparser/LogSig')#logsig __init__.py\n",
    "sys.path.append(repo_path + '/logparser/logparser/LenMa/')#for lenma __init__.py\n",
    "sys.path.append(repo_path + '/logparser/logparser/LenMa/templateminer')#for lenma\n",
    "from pipeline.pipeline import Pipeline\n",
    "from pipeline.pipeline import Parser\n",
    "from pipeline.pipeline import Loglizer\n",
    "#os.chdir('/loglizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Cdlq07gcpBF"
   },
   "source": [
    "## mining invariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjJpCNjfNgwF"
   },
   "source": [
    "### hack para probar el loglizer en pipeline,\n",
    "luego de instanciar pipeline, cambiamos los atributos, para que apunten al dataset con el que funciona mining invariants, y creamos una nueva instancia de loglizer actualmente el `loglizer_input_dir` apunta a la salida del parser que es correcto, pero hasta que no se tenga el MiningInvariants adaptado a esa entrada se rompera\n",
    "<br><br>\n",
    "###NOTA: se puede jugar y obtener cualquier intancia de parser modificando los atributos de pipeline y haciendo get_parser, lo mismo para los loglizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TY_fjAyH6XUg"
   },
   "source": [
    "### visualizamos logs dayco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "UxUiLRyE6ayx",
    "outputId": "951ab46b-5460-4f58-e69a-71d242ce26cd"
   },
   "outputs": [],
   "source": [
    "!head {repo_path}/rocore1.log\n",
    "print('******')\n",
    "!head {repo_path}/rocore2.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WY7WB4VRefF"
   },
   "source": [
    "# Pipeline\n",
    "## intentemos correr pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htOE304HtHUF"
   },
   "source": [
    "### nota importante, es deinteres seleccionar con cuidado el parser pues, de la cantidad de eventos que este descubra dependera el gasto computacional del mining invarina\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE3T9AweED-Z"
   },
   "source": [
    "### seleccionaamos la cantidade de lineas con la que trabajaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XeqYTvV7yl91",
    "outputId": "f9d8f3c4-1687-46da-fbde-14da40517fb4"
   },
   "outputs": [],
   "source": [
    "#!cp {repo_path}/rocore2.log {repo_path}/dayco_log.log\n",
    "\n",
    "#!wc -l {repo_path}/dayco_log.log\n",
    "#!ls -sh {repo_path}/dayco_log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fC5W83_Uy91S",
    "outputId": "77bfdb46-2acd-4e50-e50b-af2563d2c7ac"
   },
   "outputs": [],
   "source": [
    "#solo par auna pruea\n",
    "#!head -100 {repo_path}/rocore2.log > {repo_path}/dayco_log.log\n",
    "!wc -l {repo_path}/dayco_log.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9rxU-cxYFKZ"
   },
   "source": [
    "### NOTA: es importante que la ultima linea del archivo inicial con el que trabajeroms no contenga el salto de linea\n",
    "nosotros simplemente estamos copiando la ultima linea sin el salto de linea\n",
    "### NOTA:cuando se utilice un dataset completo tener cuidado de no darle el tratamiento que sigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPNBUcTsRFgA"
   },
   "outputs": [],
   "source": [
    "\n",
    "file = open(repo_path+'/dayco_log.log', 'r')\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "file = open(repo_path+'/dayco_log.log', 'a')\n",
    "file.write(lines[-1][:-1])\n",
    "file.close()\n",
    "file = open(repo_path+'/dayco_log.log', 'r')\n",
    "lines = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "wlIcq8rHRa4l",
    "outputId": "0202eb19-23eb-4232-84c9-4af9d616d732"
   },
   "outputs": [],
   "source": [
    "print(len(lines))\n",
    "print(lines[-2:])\n",
    "!wc -l {repo_path}/dayco_log.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0OfOYqAtuXG"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'repo_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e9213f41688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_dir\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrepo_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m  \u001b[0;31m# The input directory of log file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepo_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m  \u001b[0;31m# The output directory of parsing results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlog_file\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m'dayco_log.log'\u001b[0m  \u001b[0;31m# The input log file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#log_format = '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>'#BGL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<smonth> <sday> <shour> <ip> <id> <id2> <month> <day> <hour> <city> <type> <Content>'\u001b[0m\u001b[0;31m#dayco/rsyslog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repo_path' is not defined"
     ]
    }
   ],
   "source": [
    "input_dir  = repo_path +'/'  # The input directory of log file\n",
    "output_dir = repo_path + '/'  # The output directory of parsing results\n",
    "log_file   = 'dayco_log.log'  # The input log file name\n",
    "#log_format = '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>'#BGL\n",
    "log_format = '<smonth> <sday> <shour> <ip> <id> <id2> <month> <day> <hour> <city> <type> <Content>'#dayco/rsyslog\n",
    "\n",
    "pipeline = Pipeline(parser_algorithm='drain', input_dir = input_dir, parser_output_dir = output_dir, log_file = log_file,\n",
    "parser_regex = log_format, feature_extractor='fixed_window', log_analizer_algorithm='mining_invariants',\n",
    "data_type='time_based', elasticsearch_index_name='deepia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srxbAR3peP4n"
   },
   "source": [
    "### en time_windows el feature extracto guarda una copia de lso indices de inicio y final de cada sliding window, por eso debemos borrar ese directorio cuando vamos a probar con archivos de diferentes tamanos(si se mantien la misma configuracion de swliding window y step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NigBkjWsdGr"
   },
   "outputs": [],
   "source": [
    "!rm /dayco_log.log_structured.csv\n",
    "!rm /dayco_log.log_templates.csv\n",
    "!rm /dayco_log.logTemplateMap.csv\n",
    "!rm -r /time_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMEFthN66lSH"
   },
   "source": [
    "### Logica Online\n",
    "\n",
    "parsing\n",
    "- vamos parseando cada evento que entra???\n",
    "- esperamos que entre un minimo de logs???\n",
    "- parsemos cada cierto tiempo???\n",
    "<br> ##############<br>\n",
    "\n",
    "que hacemos con el event count matrix??\n",
    "- cada cuanto la actualizamos??\n",
    "- la guardamos en csv?\n",
    "- validamos la longitud y en funcion de si cmabio iniciamos otros procesos?\n",
    "<br> ##############<br>\n",
    "\n",
    "calculo de invarianes(tomar ne cuent que en general se tarda bastate esto)\n",
    "- cada cuanto debemos correr encontrar invariantes??\n",
    "- si sabemos que es poca data podemos correrlo cada vez que ingresa un log nuevo??\n",
    "- es practico buscar invariantes con mucha periodicidad??\n",
    "<br> ##############<br>\n",
    "\n",
    "debemos hacer inferencia con log que vayan entrando\n",
    "- esperamos que se tengan un minimo de eventos para armar un event count vector??\n",
    "- esperamos x unidades de tiempo??? \n",
    "- o con cad log probamoss??\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++++++<br>\n",
    "se me ocurre crear un metodo fisrt_time_trining, que ejecute parsing y genere una primer event_count_matrix y encuentre los primeros invariantes\n",
    "\n",
    "pedir que se evaluen todos estos enventos\n",
    "\n",
    "crear un metodo depia daemon, \n",
    "1.\n",
    "que evalue si el logfile.log incremento su tamano o si cambio la fecha del ultimo registro, calcular cual es la diferencia de lineas(guardar cual fue la ultima linea que se parseo), ejecutar parser sobre diferencia del log\n",
    "2.\n",
    "sobre la diferencia del log agregar nuevo event vector, tomar en cuenta el echo de que se utiliza sliding windows, como incluyo el evento en los logs anteriores?????? se puede/tiene que actualizar la matriz???o mejor se calcula una nueva???\n",
    "3. \n",
    "definimos un trshol de cantidad de logs para reiniciar el calculo de invariantes??\n",
    "o mas bien el treshold debe ser temporal???\n",
    "POR ULTIMO, se puede evaluar la posibilidad de ir guardando en csv un registro de invariantes???es tendria sentido????(creo que no porque me parece que los invariantes calculados solo con nuevos vectores romperan cundo se comparen con vectores viejos, ahor los viejos romperan con los nuevos??)\n",
    "4.\n",
    "partiendo de 1 y 2 deberiamos tener los nuevos vectores y simplemente podemos hacer una infeencia con los invriantes que se tengan para el momento1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3FJY1ozejUe0"
   },
   "source": [
    "## corremos el proceso de generar event_count_matrix, encontrar invariantes y detectar anomalias de data original\n",
    "el pipeline es capaz de correr sin etiquetas, con los metodos deepia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8v58tmgmzxX"
   },
   "outputs": [],
   "source": [
    "!rm {repo_path}/time_windows/sliding_3h_1h.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37092
    },
    "colab_type": "code",
    "id": "jMDyVlZVzyJa",
    "outputId": "dc17582a-0d0d-404e-f931-10635d2486cc"
   },
   "outputs": [],
   "source": [
    "#loglizer params for BGL\n",
    "para = {\n",
    "'path':repo_path + '/',                 # directory for input data\n",
    "'log_file_name':'dayco_log.log',           # filename for log data file\n",
    "'log_event_mapping':'dayco_log.logTemplateMap.csv',   # filename for log-event mapping. A list of event index, where each row represents a log\n",
    "'save_path': './time_windows/',             # dir for saving sliding window data files to avoid splitting\n",
    "#'select_column':[0,4],                      # select the corresponding columns (label and time) in the raw log file\n",
    "'select_column':[0,1,2],                      # select the corresponding columns (label and time) in the raw log file, en caso de depia no nos intersa la columna label\n",
    "'window_size':3, #3                           # time window (unit: hour)\n",
    "'step_size': 1,                             # step size (unit: hour)\n",
    "'epsilon':2.0,                              # threshold for the step of estimating invariant space\n",
    "'threshold':0.97,                           # percentage of vector Xj in matrix satisfies the condition that |Xj*Vi|<epsilon\n",
    "'scale_list':[1,2,3],\t\t\t\t\t    # list used to sacle the theta of float into integer\n",
    "'stop_invar_num':3                          # stop if the invariant length is larger than stop_invar_num. None if not given\n",
    "}\n",
    "start = time.time()\n",
    "#event_count_matrix = pipeline.log_analizer.get_event_count_matrix(para)\n",
    "#invar_dict = pipeline.log_analizer.find_invariants(para, event_count_matrix)\n",
    "#predictions, anomalies = pipeline.log_analizer.get_anomalies(para, event_count_matrix, invar_dict)\n",
    "pipeline.initial_go(para)\n",
    "end = time.time()\n",
    "print('total execution time in seconds?',end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline.run_online(para, 600, 30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11.2_pipeline_deepia_intentto_online.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
